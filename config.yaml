# Talkie speech-assist application configuration.
# All processing runs locally.

audio:
  device_id: null   # null = default system microphone
  sample_rate: 16000
  # Longer = more time to hear all words (allows pauses between words). Shorter = faster first result but may cut off.
  chunk_duration_sec: 5   # 5 = faster response; 7–10 if you need longer pauses (speech-impaired)
  # vad_min_level: when set (0.0--1.0 RMS), skip STT when chunk RMS is below this (saves CPU on silence). Set to null to disable.
  # vad_min_level: null
  # Input sensitivity (gain). 1.0 = normal; 2.0-4.0 = more sensitive for quiet speech. Adjust if too loud or too quiet.
  sensitivity: 3.0
  # Auto sensitivity: if there is some audio (above noise) but STT returns empty, increase sensitivity.
  auto_sensitivity: false   # manual only for now
  auto_sensitivity_min_level: 0.002   # RMS above this = possible speech (very low = boost)
  auto_sensitivity_max_level: 0.08    # Only boost when RMS in [min_level, this]; above = don't assume too quiet
  auto_sensitivity_step: 0.25
  auto_sensitivity_cooldown_chunks: 3 # Wait this many chunks after a bump before considering again

stt:
  # engine: vosk = faster, lower latency; whisper = best accuracy (recommended for speech-impaired users)
  engine: whisper
  # min_confidence: when set (0.0--1.0), treat transcription as empty when STT confidence is below this (Whisper only; requires transcribe_with_confidence). Set to null to disable.
  # min_confidence: 0.5
  vosk:
    # For better Vosk accuracy use a larger model, e.g. vosk-model-en-us-0.22 from https://alphacephei.com/vosk/models
    model_path: "models/vosk-model-small-en-us-0.15"
  whisper:
    # model_path: base = fastest; small = good balance; medium = best accuracy (especially impaired speech), needs ~5GB RAM
    model_path: "base"
    # no_speech_threshold: 0-1; segments with no_speech_prob above this are excluded. Set to null to disable (current behavior).
    no_speech_threshold: 0.6
    # min_avg_logprob: exclude segments with avg_logprob below this (e.g. -1 = low confidence). Set to null to disable.
    # min_avg_logprob: null

ollama:
  # Local Ollama: run `ollama serve` on the host. base_url must be http://localhost:11434.
  # (Podman Ollama service is disabled in compose.yaml and talkie-core CLI.)
  base_url: "http://localhost:11434"
  # model_name: use tinyllama (~637 MiB) if you see "model runner has unexpectedly stopped" (low RAM).
  # phi ~1.5 GiB; mistral ~4.5 GiB. Pull first: ollama pull <name>
  model_name: "tinyllama"
  # timeout_sec: request timeout for /api/generate (default 45). Increase if Ollama is slow; decrease to fail faster.
  timeout_sec: 45
  # options: pass-through to Ollama /api/generate. Defaults (in code): num_predict 256, temperature 0.4 for full single-sentence replies. Override here to tune.
  # options:
  #   num_predict: 256
  #   temperature: 0.4

# LLM prompts (system prompt and user prompt template). All prompt text is configured here.
llm:
  # Number of recent conversation turns (user + assistant pairs) to include so the LLM keeps replies in context. 0 = disabled.
  conversation_context_turns: 5
  # Minimum transcribed character length to send to the LLM. Shorter text is often noise or mishearing; skipping reduces repeated wrong phrases.
  min_transcription_length: 8
  # Minimum chunk RMS level to process (0 = disabled). Very low level + short transcription often false triggers (e.g. "you" from noise).
  min_audio_level: 0.01
  # Regeneration: raw STT -> Ollama intent sentence first. Set false for one LLM call (faster, less homophone correction).
  regeneration_enabled: true
  # When true, regenerated sentence is used as the final response when certainty >= threshold (or when certainty is not requested), skipping the second Ollama call. When false, completion is always run.
  use_regeneration_as_response: true
  # Ask regeneration to return JSON with "sentence" and "certainty" (0-100). When true, second call is skipped only when certainty >= regeneration_certainty_threshold.
  regeneration_request_certainty: true
  # Use regeneration as response only when certainty >= this (0-100). When model does not return certainty, regeneration is always used as response when use_regeneration_as_response is true.
  regeneration_certainty_threshold: 70
  # Regeneration: complete the user's partial phrase into one sentence. Do not ask "raw speech recognition" or the model may explain the task instead of completing the phrase.
  regeneration_system_prompt: |
    You complete a speech-impaired user's partial utterance into exactly one natural sentence they meant to say. The input is often fragmented or misheard (e.g. "hockey" for "I'm"). Output only that one sentence as the user would say it to a caregiver—first person for statements ("I want water.", "My leg hurts."), direct requests ("Pass me the salt."), or the question they are asking ("Do you have the time?"). No explanation, no preamble, no description of the task. If the input is already a clear, complete sentence (e.g. "Test sentence.", "I want water.", "Hello."), output that same sentence with high certainty. Only if the input is truly unintelligible noise or gibberish, output exactly: I didn't catch that. Never use "I didn't catch that" for test phrases, greetings, or clear words.
  regeneration_user_prompt_template: "Complete this phrase into one sentence the user meant to say: {transcription}"
  system_prompt: |
    You are a sentence-completion assistant for a speech-impaired user. You will receive short, partial, or fragmented speech-to-text input. Your task is to convert it into exactly ONE clear, natural, conversational sentence that expresses what the speaker likely intended to say to the person they are talking to.

    OUTPUT RULES
    - Output exactly one sentence only.
    - No explanations, no meta commentary, no extra text.
    - Do not mention “the user” or speak in third person.
    - Do not ask for clarification.
    - Do not output multiple options.
    - If input is meaningless noise or cannot reasonably be interpreted, output exactly:
      I didn't catch that.

    VOICE & PERSPECTIVE
    - Use first person for self-statements:
      "water" → "I want water."
      "cold" → "I'm cold."
      "leg hurts" → "My leg hurts."
    - For action requests directed at the listener, phrase as a direct request:
      "pass salt" → "Pass me the salt."
      "open door" → "Open the door for me."
    - If phrasing refers to “you” or the listener, or lacks “I”, prefer a question form when natural:
      "you coming" → "Are you coming?"
      "have time" → "Do you have the time?"

    COMPLETION BEHAVIOR
    - Always expand into a complete, natural sentence.
    - Keep meaning tight and minimal — do not add new topics or details.
    - Do not over-specify intent beyond what the words support.
    - Preserve original topic and vocabulary when possible.
    - Prefer the simplest natural completion.

    MISRECOGNITION CORRECTION
    Speech recognition errors are common. If a small substitution creates a clear, natural sentence, apply it:
    - “hockey” → “I’m” / “I am”
    - “outlook” → “out” (when context fits)
    - Similar phonetic fixes are allowed.
    Do NOT assume literal meanings (sports, software, objects) unless context clearly supports them.

    CONSTRAINTS
    - Never invent specific scenarios not implied by the input.
    - Never expand into multi-sentence responses.
    - Never refuse except with the exact fallback sentence above.  user_prompt_template: "Current phrase to respond to (output one sentence for this phrase only): {transcription}"
    # Short instruction for export JSONL (instruction field). If null, system_prompt is used.
    export_instruction: "You assist a speech-impaired user. Turn their partial speech into one clear, complete sentence in first person (as the user speaking: I want..., I need...). Output only that sentence."
  # Web (browse) mode: map speech to a single normalized command. When set, replaces the default JSON browse-intent prompt.
  web_mode_system_prompt: |
    You are a speech-command interpreter for a voice-controlled web browser. The user has speech difficulty and may produce short, soft, incomplete, or imperfectly recognized phrases. Your job is to map their utterance to exactly ONE valid browser command.

    You must output ONLY the normalized command string. No explanations. No extra words. No punctuation beyond what appears in the command format.

    COMMAND SET

    BROWSE MODE
    - browse on
    - browse off

    SEARCH
    - search <query>

    SAVE PAGE
    - save page

    NAVIGATION
    - back

    OPEN LINK
    - open <target>

    SCROLL
    - scroll up
    - scroll down

    CLOSE TAB
    - close
    - close tab

    INTERPRETATION RULES

    - Prefer the shortest valid command.
    - Accept soft or partial phrases and normalize them.
    - Remove filler words like "please", "can you", "would you".
    - Treat similar verbs as the same intent:
      start → browse on
      stop → browse off
      find → search
      click/select → open
      save/store → save page
      down/up alone → scroll down / scroll up
    - If the user says a command word followed by extra terms, keep the terms as parameters when valid:
      "search weather tomorrow" → search weather tomorrow
      "open result three" → open result three

    ERROR HANDLING

    - If speech is unclear but maps reasonably to a command, choose the closest valid command.
    - If no command can be determined, output exactly:
      no_command

    CONSTRAINTS

    - Output exactly one command.
    - Never ask questions.
    - Never output multiple commands.
    - Never output explanations or meta text.

# Text-to-speech: speak Ollama responses (macOS 'say' when engine: say)
# speak_timeout_sec: max seconds for one TTS utterance (default 300)
# wait_until_done_before_listen: when true, wait for TTS to finish before returning to Listening (reduces echo; default false)
# All voice options (use exact name for voice:); get current list with: say -v ?
#   en_GB: Daniel, Eddy (English (UK)), Flo (English (UK)), Grandma (English (UK)), Grandpa (English (UK)), Reed (English (UK)), Rocko (English (UK)), Sandy (English (UK)), Shelley (English (UK))
#   en_US: Albert, Bad News, Bahh, Bells, Boing, Bubbles, Cellos, Eddy (English (US)), Fred, Good News, Junior, Jester, Kathy, Ralph, Reed (English (US)), Rocko (English (US)), Samantha, Sandy (English (US)), Shelley (English (US)), Superstar, Trinoids, Whisper, Wobble, Zarvox
#   Other: Alice it_IT, Alva sv_SE, Aman en_IN, Amélie fr_CA, Amira ms_MY, Anna de_DE, Aru kk_KZ, Carmit he_IL, Damayanti id_ID, Daria bg_BG, Ellen nl_BE, Geeta te_IN, Ioana ro_RO, Jacques fr_FR, Joana pt_PT, Karen en_AU, Kyoko ja_JP, Lana hr_HR, Laura sk_SK, Lekha hi_IN, Lesya uk_UA, Linh vi_VN, Luciana pt_BR, Majed ar_001, Tünde hu_HU, Meijia zh_TW, Melina el_GR, Milena ru_RU, Moira en_IE, Mónica es_ES, Montse ca_ES, Nora nb_NO, Ona lt_LT, Organ en_US, Paulina es_MX, Piya bn_IN, Rishi en_IN, Sara da_DK, Satu fi_FI, Sinji zh_HK, Tara en_IN, Tessa en_ZA, Thomas fr_FR, Tina sl_SI, Tingting zh_CN, Vani ta_IN, Xander nl_NL, Yelda tr_TR, Yuna ko_KR, Zosia pl_PL, Zuzana cs_CZ
#   Eddy, Flo, Grandma, Grandpa, Reed, Rocko, Sandy, Shelley each have variants: (German (Germany)), (English (UK)), (English (US)), (Spanish (Spain)), (Spanish (Mexico)), (Finnish (Finland)), (French (Canada)), (French (France)), (Italian (Italy)), (Japanese (Japan)), (Korean (South Korea)), (Portuguese (Brazil)), (Chinese (China mainland)), (Chinese (Taiwan))
tts:
  enabled: true
  engine: say   # say = macOS built-in
  voice: Daniel   # default male (en_GB); override in Settings or set one of the names above
  speak_timeout_sec: 300
  wait_until_done_before_listen: false

persistence:
  db_path: "data/talkie-core.db"

# RAG: document upload, vectorization (Ollama embed), Chroma store; "Ask documents" query-by-voice.
# To run Chroma in Podman: podman compose up -d (see compose.yaml), then set chroma_host (and optional chroma_port).
rag:
  # Ollama embedding model; run: ollama pull nomic-embed-text
  embedding_model: "nomic-embed-text"
  vector_db_path: "data/rag_chroma"
  # Chroma server (e.g. in Podman). Set chroma_host to use HTTP client; leave unset for embedded DB.
  # chroma_host: "localhost"
  # chroma_port: 8000
  top_k: 5
  document_qa_top_k: 8
  chunk_size: 500
  chunk_overlap: 100
  min_query_length: 3

# Scheduled curation: pattern recognition, weighting, optional prune (run in-app or via CLI)
curation:
  # Run curator every N hours when app is running (0 = disabled)
  interval_hours: 24
  # Curation behavior
  min_weight: 0.0
  max_weight: 10.0
  correction_weight_bump: 1.5
  pattern_count_weight_scale: 0.5
  exclude_duplicate_phrase: true
  exclude_empty_transcription: true
  delete_older_than_days: null   # set to e.g. 365 to prune very old interactions
  max_interactions_to_curate: 10000

# Profile / learning: limits and display caps (used for LLM context and UI). Lower limits = less memory.
profile:
  correction_limit: 200
  accepted_limit: 50
  user_context_max_chars: 2000
  correction_display_cap: 50
  accepted_display_cap: 30
  history_list_limit: 100

ui:
  fullscreen: true
  high_contrast: true
  font_size: 24
  # Response area (large text); default 48
  response_font_size: 48

# Browser: voice-controlled web (search, open URL, store page for RAG).
browser:
  enabled: true
  # macOS: use "Google Chrome" or configurable app name for open -a
  chrome_app_name: "Google Chrome"
  fetch_timeout_sec: 20
  fetch_max_retries: 2
  search_engine_url: "https://www.google.com/search?q={query}"
  cooldown_sec: 2.0
  # Seconds after we speak (TTS) during which browse commands are ignored (avoids echo/mishear triggering clicks).
  cooldown_after_tts_sec: 12
  demo_delay_between_scenarios_sec: 4.0

logging:
  level: DEBUG   # DEBUG | INFO | WARNING | ERROR
  file: "talkie-core.log"   # app root relative; all logger output written here

# Infrastructure: Service Discovery and Caching
# Consul is the authoritative name server for internal services (*.service.consul).
# Containers use Consul DNS; the app resolves *.service.consul via Consul HTTP API.
infrastructure:
  consul:
    enabled: true
    host: "localhost"  # Override with CONSUL_HTTP_ADDR env var
    port: 8500
    token: null  # Optional ACL token

  keydb:
    enabled: true
    host: "localhost"  # Override with KEYDB_HOST env var
    port: 6379  # Override with KEYDB_PORT env var
    password: null  # Optional password
    db: 0

  service_discovery:
    enabled: true
    cache_ttl_sec: 30
    health_check_interval_sec: 30.0

  load_balancing:
    strategy: "health_based"  # round_robin, random, health_based, least_connections
    max_failover_attempts: 3

# Module server configuration
# Modules can run as HTTP API servers (default: localhost) or be accessed remotely
modules:
  speech:
    server:
      enabled: false  # Set to true to use HTTP API (default: false = in-process)
      host: "localhost"
      port: 8001
      timeout_sec: 30.0
      retry_max: 3
      retry_delay_sec: 1.0
      health_check_interval_sec: 10.0
      circuit_breaker_failure_threshold: 5
      circuit_breaker_recovery_timeout_sec: 60.0
      api_key: null  # Optional API key for remote modules
      # Service discovery options
      use_service_discovery: false  # Use Consul/KeyDB for service discovery
      endpoints: []  # Static endpoints (if not using service discovery)
      # endpoints:
      #   - "http://speech-1:8001"
      #   - "http://speech-2:8001"
      #   - "http://speech-3:8001"
  rag:
    server:
      enabled: false
      host: "localhost"
      port: 8002
      timeout_sec: 30.0
      retry_max: 3
      retry_delay_sec: 1.0
      health_check_interval_sec: 10.0
      circuit_breaker_failure_threshold: 5
      circuit_breaker_recovery_timeout_sec: 60.0
      api_key: null
      use_service_discovery: false
      endpoints: []
  browser:
    server:
      enabled: false
      host: "localhost"
      port: 8003
      timeout_sec: 30.0
      retry_max: 3
      retry_delay_sec: 1.0
      health_check_interval_sec: 10.0
      circuit_breaker_failure_threshold: 5
      circuit_breaker_recovery_timeout_sec: 60.0
      api_key: null
      use_service_discovery: false
      endpoints: []
