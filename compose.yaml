# Talkie services: Chroma runs in Podman; Ollama disabled (use local ollama serve).
# Infrastructure: Consul, KeyDB, HAProxy. Module servers: speech, rag, browser.
# mem_limit per service caps container RAM so host (and local Ollama) have headroom.
#
# The Talkie UI (main window) is NOT in a container; it runs locally.
# To see the UI: run ./talkie app (starts containers then launches the UI),
# or after "podman compose up -d" run: pipenv run python run.py
#
# Run with: ./talkie app  (recommended: starts services then opens UI)
# Or: ./talkie start  (containers only; then run "pipenv run python run.py" or ./talkie app)
services:
  # Application services
  # Ollama: disabled; use local Ollama (run: ollama serve). Uncomment block below to use Podman Ollama.
  # ollama:
  #   build:
  #     context: .
  #     dockerfile: Dockerfile.ollama
  #   image: talkie-ollama:latest
  #   container_name: talkie-ollama
  #   ports:
  #     - "11434:11434"
  #   mem_limit: 8g
  #   volumes:
  #     - ollama_data:/root/.ollama
  #   environment:
  #     - OLLAMA_PRELOAD_MODELS=tinyllama
  #   restart: unless-stopped
  #   networks:
  #     - talkie-network
  #   healthcheck:
  #     test: ["CMD", "ollama", "list"]
  #     interval: 30s
  #     timeout: 15s
  #     retries: 5
  #     start_period: 180s

  chroma:
    image: chromadb/chroma:latest
    container_name: talkie-chroma
    # Vector DB: 1g enough for typical doc sets; increase if large RAG index
    mem_limit: 1g
    ports:
      - "8000:8000"
    volumes:
      - chroma_data:/data
    restart: unless-stopped
    networks:
      - talkie-network
    depends_on:
      consul-server:
        condition: service_started
    # depends_on: ollama  # disabled; use local Ollama

  # Infrastructure: Service Discovery
  consul-server:
    image: hashicorp/consul:1.22
    container_name: talkie-consul-server
    mem_limit: 256m
    # -ui last so it takes effect after config file merge; UI at http://localhost:8500/ui
    command:
      - agent
      - -server
      - -node=server-1
      - -bootstrap-expect=1
      - -client=0.0.0.0
      - -config-file=/consul/config/consul.hcl
      - -ui
    ports:
      - "8500:8500"  # HTTP API + UI
      # DNS (port 8600) used by HAProxy resolvers; not published to host
    volumes:
      - consul_data:/consul/data
      - ./consul/consul.hcl:/consul/config/consul.hcl:ro
    restart: unless-stopped
    networks:
      talkie-network:
        ipv4_address: 172.30.0.2
    healthcheck:
      test: ["CMD", "consul", "members"]
      interval: 10s
      timeout: 3s
      retries: 3

  # Infrastructure: Cache/Registry (KeyDB - 5x faster than Redis)
  # Note: Using port 6380 to avoid conflict with existing Redis on 6379
  keydb:
    image: eqalpha/keydb:latest
    container_name: talkie-keydb
    mem_limit: 512m
    ports:
      - "6380:6379"  # External port 6380, internal 6379
    volumes:
      - keydb_data:/data
    restart: unless-stopped
    networks:
      - talkie-network
    depends_on:
      consul-server:
        condition: service_started
    command: ["keydb-server", "--appendonly", "yes"]
    healthcheck:
      test: ["CMD", "keydb-cli", "ping"]
      interval: 10s
      timeout: 3s
      retries: 3

  # Infrastructure: Reverse Proxy (HAProxy - 42k req/s)
  # Note: Using port 8080 instead of 80 for rootless Podman compatibility
  haproxy:
    image: haproxy:latest
    container_name: talkie-haproxy
    mem_limit: 128m
    command: ["haproxy", "-W", "-db", "-f", "/usr/local/etc/haproxy/haproxy.cfg"]
    # Consul DNS is on 8600; HAProxy backends use resolvers consul (172.30.0.2:8600) in haproxy.cfg
    ports:
      - "8080:8080"  # HAProxy binds 8080 inside (rootless cannot bind 80)
      - "8443:443"  # External 8443, internal 443 (if SSL configured)
      - "8404:8404"  # Stats
    volumes:
      - ./haproxy/haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
      - ./haproxy/dynamic:/usr/local/etc/haproxy/dynamic:ro
    restart: unless-stopped
    networks:
      - talkie-network
    depends_on:
      consul-server:
        condition: service_healthy
      # HAProxy will discover services via Consul once module servers register
    healthcheck:
      test: ["CMD", "haproxy", "-c", "-f", "/usr/local/etc/haproxy/haproxy.cfg"]
      interval: 10s
      timeout: 3s
      retries: 3

  # Infrastructure: Metrics (temporarily disabled)
  # victoriametrics:
  #   image: victoriametrics/victoria-metrics:latest
  #   container_name: talkie-victoriametrics
  #   ports:
  #     - "8428:8428"
  #   volumes:
  #     - victoriametrics_data:/victoria-metrics-data
  #   restart: unless-stopped
  #   networks:
  #     - talkie-network
  #   command:
  #     - "--storageDataPath=/victoria-metrics-data"
  #     - "--httpListenAddr=:8428"
  #     - "--retentionPeriod=12"
  #   healthcheck:
  #     disable: true

  # vmagent:
  #   image: victoriametrics/vmagent:latest
  #   container_name: talkie-vmagent
  #   ports:
  #     - "8429:8429"
  #   volumes:
  #     - ./victoriametrics/vmagent.yaml:/etc/vmagent/vmagent.yaml:ro
  #   restart: unless-stopped
  #   networks:
  #     - talkie-network
  #   depends_on:
  #     - victoriametrics
  #   command:
  #     - "--promscrape.config=/etc/vmagent/vmagent.yaml"
  #     - "--remoteWrite.url=http://victoriametrics:8428/api/v1/write"
  #   healthcheck:
  #     disable: true

  # grafana:
  #   image: grafana/grafana:latest
  #   container_name: talkie-grafana
  #   ports:
  #     - "3000:3000"
  #   volumes:
  #     - grafana_data:/var/lib/grafana
  #     - ./grafana/dashboards:/etc/grafana/provisioning/dashboards:ro
  #     - ./grafana/datasources:/etc/grafana/provisioning/datasources:ro
  #   restart: unless-stopped
  #   networks:
  #     - talkie-network
  #   depends_on:
  #     - victoriametrics
  #   environment:
  #     - GF_SECURITY_ADMIN_PASSWORD=admin
  #     - GF_INSTALL_PLUGINS=
  #   healthcheck:
  #     test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:3000/api/health"]
  #     interval: 10s
  #     timeout: 3s
  #     retries: 3

  # Module services (one instance each; register with Consul)
  speech:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: talkie-speech
    hostname: speech
    # STT (Whisper base/small) + TTS + speaker filter: model in RAM, 2g for Whisper base
    mem_limit: 2g
    command: ["python", "-m", "modules.speech.server", "--host", "0.0.0.0", "--port", "8001"]
    ports:
      - "8001:8001"
    volumes:
      - ./config.yaml:/app/config.yaml:ro
      - ./data:/app/data
    restart: unless-stopped
    networks:
      - talkie-network
    depends_on:
      consul-server:
        condition: service_healthy
      keydb:
        condition: service_healthy
    environment:
      # python-consul expects CONSUL_HTTP_ADDR as host:port (no http://)
      - CONSUL_HTTP_ADDR=consul-server:8500
      - KEYDB_HOST=keydb
      - KEYDB_PORT=6379
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8001/health"]
      interval: 10s
      timeout: 3s
      retries: 3

  rag:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: talkie-rag
    hostname: rag
    # Chunk + embed (Ollama external) + Chroma client; 1g sufficient
    mem_limit: 1g
    command: ["python", "-m", "modules.rag.server", "--host", "0.0.0.0", "--port", "8002"]
    ports:
      - "8002:8002"
    volumes:
      - ./config.yaml:/app/config.yaml:ro
      - ./data:/app/data
    restart: unless-stopped
    networks:
      - talkie-network
    depends_on:
      consul-server:
        condition: service_healthy
      keydb:
        condition: service_healthy
      chroma:
        condition: service_started
    environment:
      - CONSUL_HTTP_ADDR=consul-server:8500
      - KEYDB_HOST=keydb
      - KEYDB_PORT=6379
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8002/health"]
      interval: 10s
      timeout: 3s
      retries: 3

  browser:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: talkie-browser
    hostname: browser
    # Python fetch/parse; no headless Chrome in container; 512m
    mem_limit: 512m
    command: ["python", "-m", "modules.browser.server", "--host", "0.0.0.0", "--port", "8003"]
    ports:
      - "8003:8003"
    volumes:
      - ./config.yaml:/app/config.yaml:ro
      - ./data:/app/data
    restart: unless-stopped
    networks:
      - talkie-network
    depends_on:
      consul-server:
        condition: service_healthy
      keydb:
        condition: service_healthy
      # ollama:
      #   condition: service_started  # disabled; use local Ollama
    environment:
      - CONSUL_HTTP_ADDR=consul-server:8500
      - KEYDB_HOST=keydb
      - KEYDB_PORT=6379
    healthcheck:
      test: ["CMD", "wget", "--quiet", "--tries=1", "--spider", "http://localhost:8003/health"]
      interval: 10s
      timeout: 3s
      retries: 3

  # Healthbeat service: continuous health monitoring
  healthbeat:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: talkie-healthbeat
    mem_limit: 128m
    command: ["python", "-m", "modules.api.healthbeat", "--consul-host", "consul-server", "--keydb-host", "keydb", "--interval", "10.0"]
    restart: unless-stopped
    networks:
      - talkie-network
    depends_on:
      consul-server:
        condition: service_healthy
      keydb:
        condition: service_healthy
    environment:
      - CONSUL_HTTP_ADDR=consul-server:8500
      - KEYDB_HOST=keydb
      - KEYDB_PORT=6379

networks:
  talkie-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.30.0.0/24

volumes:
  # ollama_data:  # used when Podman Ollama service is enabled
  chroma_data:
  consul_data:
  keydb_data:
  # victoriametrics_data:
  # grafana_data:
